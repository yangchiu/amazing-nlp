import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
import numpy as np
import matplotlib.pyplot as plt
import os

# gan is for Generative Adversarial Network

save_dir = 'gan/'
if not os.path.exists(save_dir):
    os.makedirs(save_dir)

data_dir = 'MNIST_data'


# With a Leaky ReLU (LReLU),
# you won’t face the “dead ReLU” problem which happens when your ReLU always have values under 0
# this completely blocks learning in the ReLU because of gradients of 0 in the negative part.
def leaky_relu(tensor):
    alpha = 0.01
    return tf.maximum(alpha * tensor, tensor)


def generator(noise, reuse=None):
    #print(f'* calling {generator.__name__}')

    with tf.variable_scope('gen', reuse=reuse):
        hidden1 = leaky_relu(tf.layers.dense(inputs=noise, units=128))
        hidden2 = leaky_relu(tf.layers.dense(inputs=hidden1, units=128))
        output = tf.layers.dense(hidden2, units=784, activation=tf.nn.tanh)

        # generator would return fake images
        return output


def discriminator(x, reuse=None):
    #print(f'* calling {discriminator.__name__}')

    with tf.variable_scope('dis', reuse=reuse):
        hidden1 = leaky_relu(tf.layers.dense(inputs=x, units=128))
        hidden2 = leaky_relu(tf.layers.dense(inputs=hidden1, units=128))
        logits =  tf.layers.dense(hidden2, units=1)
        output = tf.sigmoid(logits)

        # discriminator would return the x is a real image or not
        # if it's a real image, return -> 1
        # if it's a fake image, return -> 0
        return output, logits


if __name__ == '__main__':

    mnist = input_data.read_data_sets(os.path.join(save_dir, data_dir), one_hot=True)
    print(f'=> get {mnist.train.num_examples} training data')

    # placeholders

    real_images = tf.placeholder(tf.float32, shape=[None, 784])
    noise = tf.placeholder(tf.float32, shape=[None, 100])

    # generator
    g = generator(noise)

    # discriminator
    # create a new discriminator, and feed real images into it
    d_output_real, d_logits_real = discriminator(real_images)
    # reuse the old discriminator, but feed fake images generated by g instead
    d_output_fake, d_logits_fake = discriminator(g, reuse=True)

    # loss
    def loss_func(logits_in, labels_in):
        return tf.reduce_mean(
            tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_in, labels=labels_in)
        )

    # for real images, the labels should be 1
    d_real_loss = loss_func(d_logits_real, tf.ones_like(d_logits_real) * 0.9)
    # for fake images, the labels should be 0
    d_fake_loss = loss_func(d_logits_fake, tf.zeros_like(d_logits_fake))

    d_loss = d_real_loss + d_fake_loss

    # the objective of g is to generate "fake" images and let
    # d label them as "real"
    # so the labels should be 1
    g_loss = loss_func(d_logits_fake, tf.ones_like(d_logits_fake))

    # optimizer
    learning_rate = 0.001

    tvars = tf.trainable_variables()

    d_vars = [var for var in tvars if 'dis' in var.name]
    g_vars = [var for var in tvars if 'gen' in var.name]

    print(f'=> variables for discriminator:')
    print(d_vars)
    print(f'=> variables for generator:')
    print(g_vars)

    d_train = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)
    g_train = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)

    # init
    init = tf.global_variables_initializer()

    # session
    saver = tf.train.Saver(var_list=g_vars)
    # save a sample per epoch
    samples = []
    with tf.Session() as sess:

        sess.run(init)

        batch_size = 100
        epochs = 500
        num_batches = mnist.train.num_examples // batch_size

        fig, axes = plt.subplots(nrows=5, ncols=10, figsize=(20, 10))

        for i in range(epochs):

            for j in range(num_batches):

                batch_x, _ = mnist.train.next_batch(batch_size)
                # batch_x shape = (batch_size, 784)

                batch_noise = np.random.uniform(-1, 1, size=(batch_size, 100))

                # d_fake_loss is related to g, so noise is needed
                sess.run(d_train, feed_dict={
                    real_images: batch_x,
                    noise: batch_noise
                })
                sess.run(g_train, feed_dict={
                    noise: batch_noise
                })

            print(f'=> epoch {i} of {epochs}')

            if i % 10 == 0:
                sample_noise = np.random.uniform(-1, 1, size=(1, 100))
                gen_sample = sess.run(
                    generator(noise, reuse=True),
                    feed_dict={noise: sample_noise}
                )
                samples.append(gen_sample)

                m = (i // 10) // 10
                n = (i // 10) % 10
                axes[m][n].imshow(np.reshape(samples[-1], (28, 28)))
                plt.savefig(os.path.join(save_dir, 'fig.png'))

        saver.save(sess, save_dir)


